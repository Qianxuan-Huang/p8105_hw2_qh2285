---
title: "Homework 2"
author: "Qianxuan Hunag"
date: '2024-10-01'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE)
```


```{r load_libraries}
library(tidyverse)
library(readxl)
library(ggplot2)
```


## Problem 1

Below is code of importing  `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. Change `Route` columns 8-11 to character as same as 1-7. Then update variable names, and selects the columns we need. Change `entry` from `yes` / `no` to a logical variable.

```{r}
trans_ent = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>%
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

As it stands, these data are not "tidy": route number should be a variable, as should route. That is, to obtain a tidy dataset we would need to convert `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` to obtain all unique combinations. As a result, the number of rows in this dataset is the number of unique stations. .

```{r}
trans_ent %>% 
  select(station_name, line) %>% 
  distinct()
```

The next code chunk is similar, but filters according to ADA compliance as an initial step. This produces a dataframe in which the number of rows is the number of ADA compliant stations. 

```{r}
trans_ent %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct()
```

To compute the proportion of station entrances / exits without vending allow entrance, we first exclude station entrances that do not allow vending. Then, we focus on the `entry` variable -- this logical, so taking the mean will produce the desired proportion (recall that R will coerce logical to numeric in cases like this).

```{r}
trans_ent %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. As a first step, we tidy the data as alluded to previously; that is, we convert `route` from wide to long format. After this step, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows).

```{r}
trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct()

trans_ent %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct()
```


## Problem 2

tidy up Mr. Trash Wheel sheet:
```{r}
mr_trash_wheel_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Mr. Trash Wheel", , na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(sports_balls = as.integer(round(sports_balls))) 
```
tidy up Professor Trash Wheel sheet:
```{r}
pr_trash_wheel_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Professor Trash Wheel", , na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) 

```
tidy up Gwynnda Trash Wheel sheet:
```{r}
gw_trash_wheel_df = 
  read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Gwynnda Trash Wheel",  na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) 

```
add an additional variable of sheet name:
```{r}
mr_trash_wheel_df = mutate(pr_trash_wheel_df, trash_wheel = "Mr. Trash Wheel")
pr_trash_wheel_df = mutate(pr_trash_wheel_df, trash_wheel = "Professor Trash Wheel")
gw_trash_wheel_df = mutate(pr_trash_wheel_df, trash_wheel = "Gwynnda Trash Wheel")
```
combine 3 tabels together:
```{r}
trash_wheel_df <- bind_rows(mr_trash_wheel_df, pr_trash_wheel_df, gw_trash_wheel_df)

trash_wheel_df
```

### summarizing the dataset

The number of observations in "Mr. Trash Wheel" is 263, in "Professor Trash Wheel" is 119, in "Gwynnda Trash Wheel" is 263, 789 observations in total:
```{r}
nrow(trash_wheel_df)
nrow(mr_trash_wheel_df)
nrow(pr_trash_wheel_df)
nrow(gw_trash_wheel_df)
```

key variables:
All 3 tables show weight, date, Homes Powered or not, and different kinds of trash, such as Plastic Bottles, Polystyrene,	Cigarette Butts,	Glass Bottles,	Plastic Bags,	Wrappers. And "Mr. Trash Wheel" includes extra kind	Sports Balls. 



total weight of trash collected by Professor Trash Wheel is 246.74 tons:
```{r}
total_weight_pr = pr_trash_wheel_df %>% 
  summarize(sum(weight_tons, na.rm = TRUE))

total_weight_pr

```
total number of cigarette butts collected by Gwynnda in June of 2022 is 18120:
```{r}
total_butts = gw_trash_wheel_df %>%
  filter(month == "June" & year == 2022) %>%
  summarize(sum(cigarette_butts, na.rm = TRUE))

total_butts
```




## problem 3

import, clean, tidy, and otherwise wrangle each of all datasets:
```{r}
bakers_df = 
  read_csv("./data/gbb_datasets/bakers.csv",  na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() %>%
  mutate(baker_first_name = word(baker_name, 1)) 

bakes_df = 
  read_csv("./data/gbb_datasets/bakes.csv",  na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() 

results_df = 
  read_csv("./data/gbb_datasets/results.csv",  na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() 

viewers_df =
  read_csv("./data/gbb_datasets/viewers.csv",  na = c("NA", "", "."),
             col_names = TRUE) %>% 
  janitor::clean_names() %>%
  pivot_longer(cols = starts_with("series_"), 
               names_to = "series", 
               names_prefix = "series_", 
               values_to = "viewers") %>%
  mutate(series = as.integer(series))
```

check for completeness and correctness across datasets (e.g. by viewing individual datasets and using anti_join): 
```{r}
anti_join(bakes_df, bakers_df, by = c("baker" = "baker_first_name"))
```

change all values include "Jo"
```{r}
bakes_df = mutate(bakes_df,
                  baker = ifelse(baker == "\"Jo\"", "Jo", baker))

```

merge to create a single, final dataset, and organize this so that variables and observations are in meaningful orders:
```{r}
gbb_df = 
  left_join(bakers_df, bakes_df, by = c("baker_first_name" = "baker", 
                                        "series" = "series")) %>%
  left_join(results_df, by = c("series", "baker_first_name" = "baker", "episode")) %>%
  
  select(-"baker") %>%
  arrange("baker_first_name", "series", "episode")

```

Export the result as a CSV in the directory containing the original datasets:
```{r}
write.csv(gbb_df, file = file.path("./data/gbb_datasets", "gbb_df.csv"), row.names = FALSE)
```

discuss the final dataset:






Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10:
```{r}
star_bakers_winners = gbb_df %>%
  filter(series >= 5, series <= 10, result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode, baker, result) %>%
  arrange(series, episode)

```

Import, clean, tidy, and organize the viewership data in viewers.csv. Show the first 10 rows of this dataset:
```{r}
viewers_df %>% arrange("series", "episode") %>%  
  head(10) %>%
  print()
```

average viewership in Season 1 is 2.77000. In Season 5 is 10.03930:
```{r}
avg_viewership_by_season = 
  viewers_df %>%
  group_by(series) %>%
  summarise(
    average_viewers = mean(viewers, na.rm = TRUE)
  )

avg_viewership_by_season
```

